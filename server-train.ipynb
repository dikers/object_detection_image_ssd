{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标检测  SSD 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from xml.etree import ElementTree\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# 修改成自己的根路径\n",
    "base_dir = './raw-data/'\n",
    "\n",
    "# 将图片保存到以下文件夹下面\n",
    "image_save_path = os.path.join(base_dir , 'JPEGImages/') \n",
    "annotations_save_path = os.path.join(base_dir + 'Annotations/') \n",
    "label_list = ['401', '402', '403', '201', '202']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------\n",
    "准备数据 \n",
    "=============================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第一步 准备图片\n",
    "\n",
    "录制需要识别物体的视频， 或者直接拍摄图片\n",
    "\n",
    "安装ffmpeg   \n",
    "```\n",
    " ffmpeg -ss 00:00 -i test.mov -f image2  -s 640x426 -r 1 -t 01:00 test_%3d.jpg\n",
    " \n",
    " -ss 开始时间\n",
    " -i  视频路径\n",
    " -f  类型\n",
    " -s  大小\n",
    " -r   速率， 每秒一张图片\n",
    " -t 结束时间\n",
    " 201_%3d.jpg  对图片进行命名\n",
    "\n",
    "```\n",
    "\n",
    "将所有的图片放置到一个文件夹\n",
    "```shell\n",
    "raw-data/\n",
    "    JPEGImages/   #所有图片\n",
    "    Annotations/  #所有xml标记信息\n",
    "\n",
    "```\n",
    "\n",
    "然后进行打标签工作\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第二步 打标签\n",
    "\n",
    "打标签工具下载地址\n",
    "[项目地址](https://github.com/wkentaro/labelme)\n",
    "\n",
    "![image](http://raw.githubusercontent.com/tzutalin/labelImg/master/demo/demo3.jpg)\n",
    "\n",
    "```\n",
    "Ctrl + u\tLoad all of the images from a directory\n",
    "Ctrl + r\tChange the default annotation target dir\n",
    "Ctrl + s\tSave\n",
    "Ctrl + d\tCopy the current label and rect box\n",
    "Space\tFlag the current image as verified\n",
    "w\tCreate a rect box\n",
    "d\tNext image\n",
    "a\tPrevious image\n",
    "del\tDelete the selected rect box\n",
    "Ctrl++\tZoom in\n",
    "Ctrl--\tZoom out\n",
    "↑→↓←\tKeyboard arrows to move selected rect box\n",
    "```\n",
    "\n",
    "\n",
    "分别设置图片的文件夹位置  和生成的xml保存路径， 然后进行打标签工作。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三步  上传图片和xml标签数据到服务器\n",
    "\n",
    "图片上传路径        base_dir + 'JPEGImages/' \n",
    "\n",
    "标记数据上传路径     base_dir + 'Annotations/' \n",
    "\n",
    "\n",
    "将本地图片和xml 标记数据 压缩， 然后上传到服务器上\n",
    "\n",
    "```shell\n",
    "#压缩本地文件 zip 格式\n",
    "zip raw-data.zip  raw-data/\n",
    "\n",
    "scp  -i \"~/bin/key.pem\"  raw-data.zip  ec2-user@ip_adress:/home/ec2-user/examples/object_detection_image_ssd/\n",
    "\n",
    "```\n",
    "\n",
    "登录到服务器\n",
    "\n",
    "```shell\n",
    "\n",
    "cd /home/ec2-user/examples/object_detection_image_ssd/\n",
    "\n",
    "rm -fr raw-data\n",
    "\n",
    "unzip raw-data.zip\n",
    "```\n",
    "新建好的路径如下： \n",
    "```\n",
    "object_detection_image_ssd/\n",
    "    raw-data/\n",
    "        JPEGImages/   #所有图片\n",
    "        Annotations/  #所有xml标记信息\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.25\n",
    "test_ratio = 0.05\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "imagesets_path = base_dir + 'Imagesets/'\n",
    "if not os.path.exists(imagesets_path): \n",
    "    os.mkdir(imagesets_path)\n",
    "\n",
    "layout_path = base_dir + 'Imagesets/Layout/'\n",
    "if not os.path.exists(layout_path): \n",
    "    os.mkdir(layout_path)\n",
    "\n",
    "main_path = base_dir + 'Imagesets/Main/'\n",
    "if not os.path.exists(main_path): \n",
    "    os.mkdir(main_path)\n",
    "    \n",
    "json_save_path = base_dir + 'json/'\n",
    "if not os.path.exists(json_save_path): \n",
    "    os.mkdir(json_save_path)\n",
    "\n",
    "\n",
    "# Sagemaker 所需要的数据集\n",
    "sagemaker_path = base_dir + 'sagemaker/'\n",
    "\n",
    "if not os.path.exists(sagemaker_path): \n",
    "    os.mkdir(sagemaker_path)\n",
    "\n",
    "train_path = sagemaker_path + 'train/'\n",
    "if not os.path.exists(train_path): \n",
    "    os.mkdir(train_path)\n",
    "    \n",
    "train_annotation_path = sagemaker_path + 'train_annotation/'\n",
    "if not os.path.exists(train_annotation_path): \n",
    "    os.mkdir(train_annotation_path)\n",
    "    \n",
    "validation_path = sagemaker_path + 'validation/'\n",
    "if not os.path.exists(validation_path): \n",
    "    os.mkdir(validation_path)\n",
    "    \n",
    "validation_annotation_path = sagemaker_path + 'validation_annotation/'\n",
    "if not os.path.exists(validation_annotation_path): \n",
    "    os.mkdir(validation_annotation_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第四步  划分训练 验证 测试数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XML_preprocessor(object):\n",
    "\n",
    "    def __init__(self, data_path, label_list, json_save_path):\n",
    "        self.path_prefix = data_path\n",
    "        self.num_classes = len(label_list)\n",
    "        self.data = dict()\n",
    "        self._label_list = label_list\n",
    "        self._json_save_path = json_save_path\n",
    "        self._preprocess_XML()\n",
    "\n",
    "    def _preprocess_XML(self):\n",
    "        filenames = os.listdir(self.path_prefix)\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.startswith('.'):\n",
    "                continue\n",
    "            if not filename.endswith('.xml'):\n",
    "                continue\n",
    "\n",
    "\n",
    "            tree = ElementTree.parse(self.path_prefix + filename)\n",
    "            root = tree.getroot()\n",
    "            bounding_boxes = []\n",
    "            one_hot_classes = []\n",
    "            size_tree = root.find('size')\n",
    "            width = float(size_tree.find('width').text)\n",
    "            height = float(size_tree.find('height').text)\n",
    "            for object_tree in root.findall('object'):\n",
    "                for bounding_box in object_tree.iter('bndbox'):\n",
    "                    xmin = float(bounding_box.find('xmin').text)/width\n",
    "                    ymin = float(bounding_box.find('ymin').text)/height\n",
    "                    xmax = float(bounding_box.find('xmax').text)/width\n",
    "                    ymax = float(bounding_box.find('ymax').text)/height\n",
    "                bounding_box = [xmin,ymin,xmax,ymax]\n",
    "                bounding_boxes.append(bounding_box)\n",
    "                class_name = object_tree.find('name').text\n",
    "                one_hot_class = self._to_one_hot(class_name)\n",
    "                one_hot_classes.append(one_hot_class)\n",
    "            image_name = root.find('filename').text\n",
    "            bounding_boxes = np.asarray(bounding_boxes)\n",
    "            one_hot_classes = np.asarray(one_hot_classes)\n",
    "            image_data = np.hstack((bounding_boxes, one_hot_classes))\n",
    "            self.data[image_name] = image_data\n",
    "\n",
    "    def _to_one_hot(self,name):\n",
    "        one_hot_vector = [0] * self.num_classes\n",
    "\n",
    "        _index = self._label_list.index(name)\n",
    "\n",
    "        if _index < 0:\n",
    "            print('Annotations 中的label 和配置文件中 不一致 unknown label: %s' % name)\n",
    "        one_hot_vector[_index] = 1\n",
    "        return one_hot_vector\n",
    "\n",
    "    def _save_file(self, json_object, path):\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(json_object, f)\n",
    "\n",
    "        \n",
    "    def to_json(self):\n",
    "        \"\"\"\n",
    "        生成sagemaker 使用的json 文件\n",
    "        \"\"\"\n",
    "        train_val_list = list()\n",
    "\n",
    "        filenames = os.listdir(self.path_prefix)\n",
    "\n",
    "        for filename in filenames:\n",
    "\n",
    "            if filename.startswith('.'):\n",
    "                continue\n",
    "            if not filename.endswith('.xml'):\n",
    "                continue\n",
    "\n",
    "            json_object = dict()\n",
    "            tree = ElementTree.parse(self.path_prefix + filename)\n",
    "            root = tree.getroot()\n",
    "            size_tree = root.find('size')\n",
    "            image_name = root.find('filename').text\n",
    "            json_object['file'] = image_name\n",
    "\n",
    "            width = int(size_tree.find('width').text)\n",
    "            height = int(size_tree.find('height').text)\n",
    "\n",
    "            annotations = list()\n",
    "            categories = list()\n",
    "\n",
    "            for object_tree in root.findall('object'):\n",
    "\n",
    "                annotation = dict()\n",
    "                category = dict()\n",
    "                _top = 0\n",
    "                _left = 0\n",
    "                _width = 0\n",
    "                _height = 0\n",
    "\n",
    "                for bounding_box in object_tree.iter('bndbox'):\n",
    "                    _top = int(bounding_box.find('ymin').text)\n",
    "                    _left = int(bounding_box.find('xmin').text)\n",
    "                    _width = int(bounding_box.find('xmax').text) - _left\n",
    "                    _height = int(bounding_box.find('ymax').text) - _top\n",
    "\n",
    "\n",
    "                class_name = object_tree.find('name').text\n",
    "\n",
    "                class_id = self._label_list.index(class_name)\n",
    "                if class_id < 0:\n",
    "                    print('Annotations 中的label 和配置文件中 不一致 unknown label: %s' % class_name)\n",
    "                annotation['class_id'] = class_id\n",
    "                annotation['top'] = _top\n",
    "                annotation['left'] = _left\n",
    "                annotation['width'] = _width\n",
    "                annotation['height'] = _height\n",
    "                category['class_id'] = class_id\n",
    "                category['name'] = class_name\n",
    "                train_val = dict()\n",
    "                train_val['name'] = image_name.split('.')[0]\n",
    "                train_val['label'] = class_id + 1\n",
    "                train_val_list.append(train_val)\n",
    "\n",
    "\n",
    "                annotations.append(annotation)\n",
    "                categories.append(category)\n",
    "\n",
    "            image_list = list()\n",
    "            image_size = dict()\n",
    "            image_size['width'] = width\n",
    "            image_size['height'] = height\n",
    "            image_size['depth'] = 3\n",
    "            image_list.append(image_size)\n",
    "\n",
    "            json_object['image_size']= image_list\n",
    "            json_object['annotations'] = annotations\n",
    "            json_object['categories'] = categories\n",
    "\n",
    "            path = self._json_save_path + image_name.split('.')[0] +'.json'\n",
    "            self._save_file(json_object, path)\n",
    "\n",
    "        return train_val_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xml = XML_preprocessor(annotations_save_path, label_list, json_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = xml.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "data_length = len(all_data)\n",
    "random.shuffle(all_data)\n",
    "print('样本数量    :', len(all_data))\n",
    " \n",
    "\n",
    "val_count = int(data_length * val_ratio)\n",
    "test_count = int(data_length * test_ratio)\n",
    "\n",
    "val_list =  all_data[0:val_count]\n",
    "test_list =  all_data[val_count: val_count+test_count]\n",
    "train_list = all_data[val_count+test_count:]\n",
    "\n",
    "print('验证集数量  :', len(val_list))\n",
    "print('测试集数量  :', len(test_list))\n",
    "print('训练集数量  :', len(train_list))\n",
    "\n",
    "\n",
    "def dict_to_set(list_dict):\n",
    "    result_set = set()\n",
    "    for i in list_dict:\n",
    "        result_set.add(i['name'])\n",
    "    return result_set\n",
    "\n",
    "\n",
    "dict_to_set(val_list)\n",
    "\n",
    "def write_list_to_file(_list, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for i in _list:\n",
    "            f.write('{} {}\\n'.format(i['name'], i['label']))\n",
    "\n",
    "def write_set_to_file(_set, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        for i in _set:\n",
    "            f.write('{}\\n'.format(i))            \n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_list_to_file(train_list, layout_path + 'train.txt')\n",
    "write_list_to_file(val_list, layout_path + 'trainval.txt')\n",
    "write_list_to_file(test_list, layout_path + 'val.txt')\n",
    "\n",
    "write_set_to_file(dict_to_set(train_list), main_path + 'train.txt')\n",
    "write_set_to_file(dict_to_set(val_list), main_path + 'trainval.txt')\n",
    "write_set_to_file(dict_to_set(test_list), main_path + 'val.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第五步   将数据写入到sagemaker文件夹里， 供sagemaker训练使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "train_set = dict_to_set(train_list)\n",
    "val_set = dict_to_set(val_list)\n",
    "test_set = dict_to_set(test_list)\n",
    "\n",
    "for item in train_set:\n",
    "    copyfile('{}{}.jpg'.format(image_save_path, item) ,'{}{}.jpg'.format(train_path, item) )\n",
    "    copyfile('{}{}.json'.format(json_save_path, item) ,'{}{}.json'.format(train_annotation_path, item) )\n",
    "    \n",
    "\n",
    "for item in val_set:\n",
    "    copyfile('{}{}.jpg'.format(image_save_path, item) ,'{}{}.jpg'.format(validation_path, item) )\n",
    "    copyfile('{}{}.json'.format(json_save_path, item) ,'{}{}.json'.format(validation_annotation_path, item) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------\n",
    "开始训练\n",
    "==================================================================\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping\n",
    "import pickle\n",
    "\n",
    "\n",
    "from SSD300.ssd_v2 import SSD300v2\n",
    "from SSD300.ssd_training import MultiboxLoss\n",
    "from SSD300.ssd_utils import BBoxUtility\n",
    "\n",
    "from SSD300.get_data_from_XML import XML_preprocessor\n",
    "from SSD300.generator import Generator\n",
    "\n",
    "NUM_CLASSES = len(label_list) + 1\n",
    "input_shape = (300, 300, 3)\n",
    "\n",
    "model = SSD300v2(input_shape, num_classes=NUM_CLASSES)\n",
    "\n",
    "loss = MultiboxLoss(NUM_CLASSES, neg_pos_ratio=2.0).compute_loss\n",
    "model.compile(optimizer='Adadelta', loss=loss)\n",
    "\n",
    "\n",
    "priors = pickle.load(open('./SSD300/prior_boxes_ssd300.pkl', 'rb'))\n",
    "bbox_util = BBoxUtility(NUM_CLASSES, priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_voc_07_parser = XML_preprocessor(data_path=annotations_save_path , label_list=label_list)\n",
    "# len(pascal_voc_07_parser.data) = 5011\n",
    "\n",
    "# pascal_voc_07_parser.data['000007.jpg']\n",
    "# array([[ 0.282     ,  0.15015015,  1.        ,  0.99099099,  0.        ,\n",
    "#          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "#          1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "#          0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
    "#          0.        ,  0.        ,  0.        ,  0.        ]])\n",
    "\n",
    "keys = list(pascal_voc_07_parser.data.keys())\n",
    "train_num = int(0.7 * len(keys))\n",
    "train_keys = keys[:train_num]\n",
    "val_keys = keys[train_num:]\n",
    "\n",
    "gen = Generator(gt=pascal_voc_07_parser.data, bbox_util=bbox_util,\n",
    "                 batch_size=16, path_prefix= image_save_path,\n",
    "                 train_keys=train_keys, val_keys=val_keys, image_size=(300, 300))\n",
    "\n",
    "\n",
    "keys = list(pascal_voc_07_parser.data.keys())\n",
    "print(keys[0:10])\n",
    "print(pascal_voc_07_parser.data[keys[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN = RUN + 1 if 'RUN' in locals() else 1\n",
    "\n",
    "LOG_DIR = './output/run{}'.format(RUN)\n",
    "LOG_FILE_PATH = LOG_DIR + '/checkpoint-{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=LOG_DIR, write_images=True)\n",
    "checkpoint = ModelCheckpoint(filepath=LOG_FILE_PATH, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "history = model.fit_generator(generator=gen.generate(True), steps_per_epoch=int(gen.train_batches ),\n",
    "                              validation_data=gen.generate(False), validation_steps=int(gen.val_batches),\n",
    "                              epochs=EPOCHS, verbose=1, callbacks=[tensorboard, checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------\n",
    "测试模型\n",
    "==========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('./output/run1/checkpoint-08-0.7219.hdf5', by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取测试数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from scipy.misc import imread\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "filename = base_dir + 'Imagesets/Main/val.txt'\n",
    "files = []\n",
    "with open(filename, \"r\", encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.split('\\n')[0]\n",
    "        line = base_dir+\"JPEGImages/{}.jpg\".format(str(line))\n",
    "        print(line)\n",
    "        files.append(line)\n",
    "        \n",
    "inputs = []\n",
    "images = []\n",
    "\n",
    "for f in files:\n",
    "    img = image.load_img(\"\"+f, target_size=(300, 300))\n",
    "    img = image.img_to_array(img)\n",
    "    \n",
    "    images.append(imread(f))\n",
    "    inputs.append(img.copy())\n",
    "\n",
    "inputs = preprocess_input(np.array(inputs))\n",
    "\n",
    "preds = model.predict(inputs, batch_size=1, verbose=1)\n",
    "# preds.shape (5, 7308, 33)\n",
    "\n",
    "results = bbox_util.detection_out(preds)\n",
    "# type(results): list, len(results): 5, len(result[0]): 200, results[0].shape: (200, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    # Parse the outputs.\n",
    "    det_label = results[i][:, 0]\n",
    "    det_conf = results[i][:, 1]\n",
    "    det_xmin = results[i][:, 2]\n",
    "    det_ymin = results[i][:, 3]\n",
    "    det_xmax = results[i][:, 4]\n",
    "    det_ymax = results[i][:, 5]\n",
    "\n",
    "    # Get detections with confidence higher than 0.6.\n",
    "    top_indices = [i for i, conf in enumerate(det_conf) if conf >= 0.3]\n",
    "\n",
    "    top_conf = det_conf[top_indices]\n",
    "    top_label_indices = det_label[top_indices].tolist()\n",
    "    top_xmin = det_xmin[top_indices]\n",
    "    top_ymin = det_ymin[top_indices]\n",
    "    top_xmax = det_xmax[top_indices]\n",
    "    top_ymax = det_ymax[top_indices]\n",
    "\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, 21)).tolist()\n",
    "\n",
    "    plt.imshow(img / 255.)\n",
    "    currentAxis = plt.gca()\n",
    "\n",
    "    for i in range(top_conf.shape[0]):\n",
    "        xmin = int(round(top_xmin[i] * img.shape[1]))\n",
    "        ymin = int(round(top_ymin[i] * img.shape[0]))\n",
    "        xmax = int(round(top_xmax[i] * img.shape[1]))\n",
    "        ymax = int(round(top_ymax[i] * img.shape[0]))\n",
    "        score = top_conf[i]\n",
    "        label = int(top_label_indices[i])\n",
    "        label_name = label_list[label - 1]\n",
    "        display_txt = '{:0.2f}, {}'.format(score, label_name)\n",
    "        coords = (xmin, ymin), xmax-xmin+1, ymax-ymin+1\n",
    "        color = colors[label]\n",
    "        currentAxis.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "        currentAxis.text(xmin, ymin, display_txt, bbox={'facecolor':color, 'alpha':0.5})\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(Image), Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
